# 6.5 Multiple-Processor Scheduling
---
 
지금까지의 논의는 single processor를 가진 시스템에서의 CPU 스케줄링 문제에 집중했다. multiple CPU를 이용할 수 있다면, `load sharing`이 가능하지만 스케줄링 문제는 그만큼 복잡해질 것이다. 많은 가능성이 시도되었고 single-processor CPU scheduling에서 보았듯이 최상의 해답은 존재하지 않는다.

여기에서 multiprocessing scheduling에 대한 논의를 한다. 기능적으로 processor가 모두 동일한 시스템에 집중한다. 큐에 있는 프로세스를 실행하기 위해서 그 어떤 processor를 사용할 수 있다. 하지만 동일한 processor라도 스케줄링에 대한 제한이 있을 것이다. processor의 비공개 버스에 붙은 I/O device를 가진 시스템을 생각하자. 이 device를 사용하기 원하는 프로세스는 해당 processor에서 실행되기 위해 스케줄링 되어야만 한다.

## 6.5.1 Approaches to Multiple-Processor Scheduling
---

multiprocessor system에서 CPU scheduling을 위한 한 가지 접근법은, master server라고 불리는 single processor에 의해 핸들되는 스케줄링 결정, I/O 처리, 다른 시스템 활동을 갖는다. 다른 processor는 user code만을 실행한다. 이 `asymmetric multiprocessing`은, 오직 하나의 processor만 system data structure에 접근하고 데이터 공유에 대한 필요를 감소시키기 때문에 단순하다.

두 번째 접근법은 각 processor가 스스로 스케줄링 하는 `symmetric multiprocessing`(`SMP`)을 사용하는 것이다 모든 프로세스는 보통의 ready queue에 위치할 것이다. 각 processor는 ready 프로세스의 비공개 큐를 가지고 있을 것이다. 관계없이, 각 processor에 대한 스케줄러가 ready queue를 선택하고 실행될 프로세스를 선택하여 스케줄링이 진행된다. 5장에서 보았듯이, common data structure에 액세스하고 업데이트하려는 multiple processor가 있다면 스케줄러는 주의깊게 프로그램되어야 한다. 우리는 별도의 두 processor가 같은 프로세스를 스케줄링하려고 선택하지 못하도록, 프로세스가 큐에서 사라지지 않도록 보장해야 한다. 사실상 Windows, Linux, Mac OS X를 포함하는 모든 현대 os는 SMP을 지원한다. 남은 절에서 SMP 시스템과 관련된 이슈를 논의한다.

## 6.5.2 Processor Affinity
---

프로세스가 특정 processor에서 실행되었을 때 캐시 메모리에는 어떤 일이 발생하는지 생각해보자. 프로세스에 의해 가장 최근 액세스된 데이터는 processor에 대한 캐시에 채워진다다. 결국, 프로세스에 의한 연속적인 메모리 액세스는 캐시 메모리에서 만족된다.(만족?) 이제 프로세스가 다른 processor로 옮겨간다면 어떤 일이 발생하는지를 생각하자. 캐시 메모리의 내용은 처음 processor에 무효화되어야 한다. 그리고 두 번째 processor에 대한 캐시를 다시 채워야 한다. 이 무효화와 다시 채우는 비용이 비싸기 때문에 대부분의 SMP 시스템은 프로세스의 이주는 피하려 하고 같은 processor에서 실행 중인 프로세스를 유지하려고 한다. 이것을 `processor affinity`라고 한다. 이것은 한 프로세스가 현재 실행 중인 processor에 대한 친밀도를 가지게 한다.

processor 친밀도는 몇가지 형태를 갖는다. os가 같은 processor에서 실행 중인 프로세스를 유지하려는 정책(성공하지는 못 하더라도)을 가지는 상황을 `soft affinity`라고 한다. os가 single processor에서 프로세스를 유지하려고 하지만 processor 간 이주의 가능성이 존재한다. 대조적으로 어떤 시스템에서는 `hard affinity`를 지원하는 system call을 제공하여 한 프로ㅔ스가 실행될 processor의 부분 집합을 명시하도록 한다. 많은 시스템은 soft, hard affinity를 모두 제공한다. 예를 들어, Linux는 soft affinity를 구현하지만, hard affinity를 지원하는 *sched_setaffinity()* system call을 제공한다.

시스템의 메인 메모리 아키텍처는 processor affinity에 영향을 줄 수 있다. [Figure 6.9, p281]은 CPU가 메인 메모리의 일부에 빠르게 액세스하는, non-uniform memory access(NUMA)의 특징을 가진 아키텍처를 설명한다. 일반적으로 이것은 CPU와 메모리 보드가 결합된 시스템에서 일어난다. 보드 위의 CPU는, 시스템의 다른 보드에 있는 메모리에 액세스하는 것보다 빠르게 해당 보드에 액세스할 수 있다. os의 CPU 스케줄러와 memory-placement 알고리즘이 함께 동작한다면 특정 CPU에 대한 친밀도를 할당 받는 프로세스는 CPU가 위치한 보드의 메모리를 할당 받을 수 있다. 이 예는 os 책에서 설명된 것을 정확히 정의하거나 구현하지 않음을 보여주기도 한다. 오히려 os의 각 부분 간 실선은 종종 성능과 안정성을 최적화 하는 것을 목표로 하는 방법으로 연결을 만드는 알고리즘을 갖는 점선이 된다.

## 6.5.3 Load Balancing
---

SMP 시스템에서 하나 이상의 processor를 갖는 장점을 최대한 활용하기 위해서 모든 프로세스 간 균형잡힌 workload를 유지하는 것이 중요하다. 그렇지 않으면 processor가 CPU를 기다리는 프로세스 목록과 함께 높은 workload를 갖는 동안, 하나 이상의 processor가 idle상태로 있을 것이다. `Load balancing`은 SMP 시스템에서 모든 processor에 고르게 분산된 workload를 유지하려 한다. 일반적으로 load balancing은, 각 processor가 실행에 적합한 프로세스의 비공개 큐를 가지고 있는 시스템에서만 요구되는 것을 주목하자. 보통의 실행 큐를 가진 시스템에서는 load balancing은 필요하지 않다. 이것은 processor가 idle 상태가 되면, 즉시 보통의 실행 큐에서 실행 가능한 프로세스를 추출하기 때문이다. 하지만 SMP를 지원하는 대부분의 현대 os에서 각 processor는 적합한 프로세스의 비공개 큐를 갖는다.

load balancing을 위한 두 가지 접근법 `push migration`, `pull migration`이 있다. push migration에서는 특정 태스크가 각 processor의 로드를 주기적으로 체크하고 imbalance가 발견된다면 오버로드된 프로세스에서 idle프로세스로 이동 (또는 푸시)함으로써 고르게 분산시킨다. push migration은 idle processor가 바쁜 processor로부터 대기 중인 태스크를 가져올 때 발생한다. push, pull migration은 서로 배타적일 필요가 없고, 종종 load-balancing 시스템에서 병렬적으로 구현된다. 예로써, 6.7.1절에 설명된 Linux 스케줄러와, FreeBSD시스템에서 이용되는 ULE 스케줄러는 이 두 기술을 모두 구현한다.

흥미롭게도, load balancing은 종종 6.5.2절에서 설명된 processor affinity의 이점을 거스른다. 즉, 같은 processor에서 실행 중인 프로세스를 유지하는 이점은, 프로세스가 processor의 캐시 메모리로 들어가는 데이터를 이용할 수 있다는 것이다. 하나의 processor에서 다른 processor로 프로세스를 pull 또는 push하는 것은 이런 이점을 제거한다. 시스템 엔지니어링에서는 흔하지만 최적의 정책에 대한 규칙은 없다. 그러므로 어떤 시스템에서는, idle processor가 항상 non-idle processor로부터 프로세스를 pull할 수 있다. 다른 시스템에서는 imbalance가 특정 문턱을 넘을 때만 프로세스가 이동한다.

## 6.5.4 Multicore Processor
---

전통적으로 SMP 시스템은 물리적인 multiple processor를 제공함으로써 여러 스레드가 동시에 실행되는 것을 허용했다. 그러나 최근 컴퓨터 하드웨어 분야에서는 같은 물리적인 칩에 multiple processor core를 패치해왔고 `multicore processor`를 만들었다. 각 코어는 구조적 상태를 유지하고 별도의 물리적 processor가 되기 위해 os에 나타난다. multicore processor를 사용하는 SMP 시스템은, 각 processor가 자신의 물리적인 칩을 가지고 있는 시스템보다 더 빠르고 낮은 전력을 소모한다.

multicore processor는 스케줄링 이슈를 복잡하게 한다. 어떤 일이 발생할 수 있는지 생각해보자. 연구자들은 processor가 메모리에 액세스할 때 데이터를 이용할 수 있기를 기다리는 시간을 상당히 사용하는 것을 발견했다. `memory stall`이라고 부르는 이 상황은, cache miss(캐시 메모리에 존재하지 않는 데이터에 접근하는 것) 같은 다양한 이유로 발생한다. [Figure 6.10, p282]는 memory stall을 설명한다. 이런 상황에서 processor는, 데이터가 메모리에서 이용할 수 있을 때까지 기다리는 시간의 최대 50 퍼센트를 사용할 수 있다. 이런 상황을 해결하기 위해서 많은 하드웨어 설계는, 두 개 이상의 하드웨어 스레드가 각 코어에 할당되는 multithreaded processor core를 구현했다. 이 방법은, 하나의 스레드가 메모리를 기다리는 동안 지연되면 코어가 다른 스레드로 스위치할 수 있다. [Figure 6.11, p282]는, 스레드 0과 스레드 1의 실행이 인터리브된  dual-threaded processor 코어를 설명한다. os의 관점으로는 각 하드웨어 스레드는 소프트웨어 스레드를 실행하기 위해 이용되는 논리적인 processor로 나타난다. 그러므로 dual-threaded, dual-core 시스템에서 네 개의 논리적인 processor는 os에 존재한다. UltraSPARC T3 CPU는 칩당 16개의 코어와 코어당 8개의 스레드를 가지고 있다. os의 관점으로는 128개의 논리적인 processor가 있는 것이다.

일반적으로 processor core를 멀티스레딩하기 위한 방법은 `coarse-grained`, `fine-grained` 두 방법이 존재한다. coarse-grained 멀티스레딩에서는, memory stall같은 long-latency event가 발생할 때까지 하나의 스레드가 하나의 processor에서 실행된다. long-latency event에 의해 발생한 지연때문에 processor는 다른 스레드를 실행하기 위해 스위치해야 한다. 하지만 스레드 간 스위치 비용은 비싸다. 다른 스레드가 processor core에서 실행을 시작할 수 있기 전에 instruction pipeline은 flush되어야 한다. 새로운 스레드가 실행을 시작한다면 이 스레드는 pipeline을 자신의 instruction으로 채울 것이다. fine-grained(또는 interleaved) 멀티스레딩은, 더 세분화된 곳에서(일반적으로 instruction cycle의 경계) 스레드 간 스위치를 한다. 하지만 fine-grained 시스템의 구고적 설계는 스레드 스위칭에 대한 논리를 포함한다. 결과로서 스레드 간 스위칭 비용이 적다.

사실 멀티스레딩 multicore processor는 두 개의 다른 스케줄링 레벨을 요구 한다. 한 가지 레벨은, os가 소프트웨어 스레드를 각 하드웨어 스레드(논리적인 processor)에서 실행시키기려 선택하는 것에 의해 생성되어야 하는 스케줄링 결정이다. os는 이 스케줄링 레벨에 대해서 6.3절에서 설명한 것과 같은 스케줄링 알고리즘을 선택한다. 다른 스케줄링 레벨은 각 코어가 실행을 위한 하드웨어 스레드를 어떻게 결정하는지 명시한다. 이 상황을 수용하기 위한 몇 가지 전략이 있다. 언급했던 UltraSPARC T3는 각 코어에 대해 8개의 하드웨어 스레드를 스케줄링하기 위해 단순한 round-robin 알고리즘을 사용한다. 다른 예로 Intel Itanium은 코어당 두 개의 하드웨어 스레드를 갖는 dual-core processor이다. 각 하드웨어 스레드에 할당된 것은 0부터 7 범위의 동적인 *urgency* 값이다. 0은 가장 낮은 urgency를, 7을 가장 높은 urgency를 나타낸다. Itanium은 스레드 스위치를 트리거하는 다섯 개의 이벤트를 식별한다. 이 중 하나의 이벤트가 발생하면 thread-switching 로직은 두 스레드의 urgency를 비교하고 높은 urgency 값을 가진 스레드를 processor 코어에서 실행하기 위해  선택한다.
